\documentclass[12pt,a4paper,man]{apa7}
\usepackage{listings}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{dirtree}
\usepackage{times} % Use Times New Roman font
\usepackage[style=apa]{biblatex}

\addbibresource{bibliography.bib}

\author{Florian Donnelly}
\title{The Creation of a Programming Language}
\authorsaffiliations{Gymnasium Burgdorf}
\shorttitle{Creation of a Programming Language}
\abstract{
    This is an abstract...
}

% Defining custom commands

% include C code from file #1 from lines #2 to #3
\newcommand{\code}[3] {
    \lstset{language=C,numbers=left,basicstyle=\scriptsize,
        frame=single,caption=#1,keywordstyle=\color{blue},
        stringstyle=\color{green},commentstyle=\color{gray},
        morecomment=[l][\color{magenta}]{\#}}
    \lstinputlisting[firstnumber=#2,firstline=#2,lastline=#3]{#1}
}

\begin{document}

\onehalfspacing
\maketitle\tableofcontents\newpage

% the name of the programming language
\newcommand{\name}{\emph{TestScript }}

\section{Introduction}
\subsection{Relevance of the Topic}
\subsection{Goals}

\section{Theory}
\subsection{What this is and what this is not}
The final product in the process of creating this programming language will 
not be perfect, or in fact, be used or adopted by anyone at all, and it is not
intended to be. Creating a good, or even usable, programming language 
implementation by today's  standards means to create an entire ecosystem of 
tools, not only including a compiler or interpreter, but also a package manager
for dealing with dependencies and a platform for people to share their work.

The process used in this paper is more so a very core implementation of what
a programming language, only containing the very necessary features.

\subsection{Different types of interpreters}
In computing, there are several types of interpreters that behave differently.
Here are some examples and their use-cases:
\begin{itemize}
    \item \emph{Just in time Compiler.} JIT-compilers blur the line between
        classical interpreters and compilers. As the name suggests, code is
        being compiled at run time of the program. The JIT-compiler can optimize
        compilation of more-often used code and therefore adapt itself to
        behave somewhat close to optimally for a given use-case. Today, this 
        technique is often used instead of classical interpreters, as
        JIT-compilers come with only advantages. The most popular example of
        a JIT-compiler would be Google's V8\footcite{V8} JavaScript engine used
        in NodeJS and any Chromium browsers.
    \item \emph{Bytecode Interpreter.} Such interpret the bytecode that was
        output by a bytecode compiler from a given piece of code. The bytecode
        is an intermediate representation of the program, used to speed up
        both compilation and interpretation. Such an intermediary bytecode is
        useful for ensuring platform independence or portability, as it will be
        the same across any machine, the only difference being the interpreter.
        The most popular example would be the Java Virtual Machine\footcite{jvm},
        a bytecode interpreter for java bytecode, also used by many other languages
        such as Kotlin, Scala or Groovy.
    \item \emph{Abstract Syntax Tree interpreter.} Source code can be
        transformed into an AST or parse tree by a parser. A simple example
        can be found under 
        \hyperref[simple interpreter]{'Implementation/The Art of Abstraction'}
        on page \pageref{simple interpreter}. Compared to bytecode interpreters
        there is a large time overhead when performing syntax related computation
        or visiting tree nodes recursively.
    \item \emph{Self interpreter.} These are interpreters that interpret the
        language they themselves are written in. This requires a program
        written in another language running an interpreter for the wanted
        language, in which runs another interpreted for and written in said
        language. Using a host language to initiate such system is called
        bootstrapping.

        Self-interpreters are closely related to self-hosting compilers. These
        are in turn compilers which are written in the language they compile.
        An example of a compiler written in itself is rustc, the Rust
        compiler\footcite{rustc}, which was bootstrapped by the OCaml language.
\end{itemize}
This paper will implement a mixture form of an AST compiler and bytecode
interpreter in-one.

\section{Implementation}
\subsection{Used Software}
At the heart of creating a program in C stands the C compiler. This project uses
\emph{gcc}, the GNU Compiler Collection, which, as the name suggests, also 
supports different languages than C. Substantiating the development process is 
\emph{GNU Make}, a general-purpose build system. It handles tasks like building
the executable step-by-step from the project source code and directory clean-up.
The C ecosystem is very easy to use and fundamentally supported by 
\emph{GNU+Linux}, the operating system used to program on.
The main code editor used was \emph{Microsoft Visual Studio Code}, providing great
features like built-in git integration.
\emph{Git} is the project management and tracking software used to back up and share
the project files on \emph{GitHub.com}, a Microsoft hosted git repository server to
store and collaborate on source code.

\subsection{The Art of Abstraction}
When dealing with problems in general, a helpful method of going about can be
to create an abstraction. This is also the case when building a programming
language interpreter. Like that, an interpreter is often times subdivided into several
components, each solving a discrete problem. One way to define
such components could be the following:
\begin{enumerate}\label{simple interpreter}
    \item Create meaningful groups of characters from the input string. This
        process is called tokenizing and is done with the component called
        tokenizer or lexer. As an example, the string 'aa+  bb' could result
        in three tokens, namely 'aa', '+' and 'bb', in case space characters
        are considered unimportant.
    \item Ordering the tokens into a so-called parse-tree or syntax-tree.
        Creating an order is important, as some calculations depend on others, e.g. multiplication
        is always done before addition. TODO:example
    \item Technically, the parse tree could now be interpreted directly, and
        an output could be produced, as the interpreter know knows what to do
        and in what order to perform computations.
\end{enumerate}
As seen above, these components, or stages as they will be called in this paper,
each input and output data, where the output of one stage is the input of the
respective next stage.

\subsection{A Project Overview}

\newpage\subsubsection{File Structure}
Here is a file tree overview of the project which will be referenced
several times in the following sections of this paper. In the depicted file tree
only source files are shown, no build files or executables.
\small{\setlength{\DTbaselineskip}{10pt}\dirtree{%
    .1 project root.
    .2 .gitignore.
    .2 README.md.
    .2 LICENSE.
    .2 Makefile.
    .2 mapop.gperf.
    .2 paper.tex.
    .2 bibliography.bib.
    .2 pseudocode
        .3 parser.c
    .2 src.
        .3 main.c.
        .3 interpreter.
            .4 bytecode.c.
            .4 bytecode.h.
            .4 error.
                .5 error.c.
                .5 error.h.
            .4 interpreter.c.
            .4 interpreter.h.
            .4 libraries.
                .5 libraries.c.
                .5 libraries.h.
                .5 stdlib.
                    .6 stdlib.c.
                    .6 stdlib.h.
            .4 localizer.c.
            .4 localizer.h.
            .4 mappings.
                .5 operations.c.
                .5 operations.h.
                .5 mapop.c.
                .5 mapop.h.
            .4 memory.
                .5 hashtable.c.
                .5 hashtable.h.
                .5 array.c.
                .5 array.h.
                .5 stack.c.
                .5 stack.h.
            .4 parser.c.
            .4 parser.h.
            .4 processing.
                .5 implementations.c.
                .5 implementations.h.
            .4 runtime.c.
            .4 runtime.h.
            .4 tokenizer.c.
            .4 tokenizer.h.
}}

As visible in the file tree, most of the source-code files, ending in .c,
have a corresponding header file ending in .h. This is due to the nature
of the compilation process of the C language, which is the host language
the interpreter of \name is written in.
Each header file contains only information about functions, types or variables
available to other source or header files. The actual implementations or values
are stored in the respective .c files.
TODO:explain C compilation or link example

The pseudocode directory contains only examples used in this paper, no acutal,
working code. The paper is written in \LaTeX and BibLaTeX and compiled with
the tools pdflatex and biber. The paper's source files are paper.tex and
bibliography.bib.

The file mapop.gperf is a file to generate a perfect-hash-function\footcite{hash}
with the GNU gperf tool. The generated mapop.c file is used to map string operators
like '==' or '+' to a simpler number representation in constant time.

\subsubsection{Interpreter Construction}
This section will explain the process of constructing and the inner workings of
the \name interpreter. It is therefore the most complicated yet detailed section
in this paper. Examples will be made with both pseudo code and actual source
code from the project to further illustrate the approaches made. File paths
used are relative to the project's root directory.

The syntax of \name is not specified in a formal programming language grammar,
but rather implemented from scratch in the files src/interpreter/tokenizer.c 
and src/interpreter/parser.c.

\subsubsection{The Entry-Point}
The entry point of the project lies in the src/main.c file.
The file implements the C main function, which is the entry-point of any
C application in general. Since the file is so small, here is its source code:
\code{src/main.c}{1}{999}

For the readers who do not know how to read source code, C in particular, here is a short
explanation of the basic syntax of the C language based on the src/main.c file:
\begin{itemize}
    \item \emph{Preprocessor Instructions.} Lines beginning with hashtags are
        C preprocessor instructions, they are dealt with by the C preprocessor.
        Preprocessing is what the compiler does before actually compiling the
        code.

        The most important preprocessor instructions are '\#include <header.h>'
        to include various definitions from a header file and '\#define A B' to
        define A as an alias for B.
    \item \emph{Functions.} Functions in C are defined the following way:
        '<return type> <function name>([arguments]) { <function body> }'.
        In main.c there is one function. The function's name is 'main', its
        return type is 'int', it has two arguments 'argc' and 'argv' and a
        function body containing what the function does.
    \item \emph{Pointers.} Pointers are variables that store memory addresses
        of data. In C-like syntax they are defined with the star (*) symbol.
        An example from src/main.c is 'FILE *input;'. That means that input is not
        actually a FILE, but rather the memory address to where a FILE is.

        To dereference the pointer, meaning to actually retrieve the value stored
        at the memory address it contains, another star could be used as such:
        'FILE value = *input;'.
\end{itemize}

For those who are further interested in the C programming language, there are
many internet articles or tutorials going about reading and understanding or
writing and compiling C code. One such example is the article 'The C Beginner's
Handbook' provided by freeCodeCamp.org linked in the sources\footcite{freeCodeCamp}
on page \pageref{bibliography}.

Here is a very quick summary of what happens in the src/main.c file: the src/interpreter/interpreter.h
header file is included. In the main function, the program decides to either
read a program from the standard input or from a file. Then, the interpret function
from the said included header is called with the selected input.

\subsubsection{The Interpreter}
This section deals with the file src/interpreter/interpreter.c which guides
input data through the various stages of the \name interpreter in its
function 'interpret' which is called from the program's main function.
This section does not go in detail on the following stages, see their own sections.

The 'interpret' function calls the parser to generate a syntax-tree from tokens retrieved
from the tokenizer implemented in src/interpreter/parser.c and 
src/interpreter/tokenizer.c respectively.

After that, variables with names are 'localized', meaning their names are
replaced by addresses where their value is stored by src/interpreter/localizer.c.
The advantage of performing this step so early is that mapping variable names
to memory is a rather resource-intensive task with time complexity of maximally 
O(log n), n being the amount of different variables.

The syntax tree could now be interpreted directly, but is instead converted
to an intermediary internal bytecode format by src/interpreter/bytecode.c.
This step is done so that code that is ran multiple times does not convey
the problem of time overhead when traversing the syntax tree, but the current
implementation does not take advantage of this.

The final step is to process or execute said bytecode. This step is performed
by src/interpreter/runtime.c.

\subsubsection{The Tokenizer}
The tokenizer is implemented in src/interpreter/tokenizer.c. Its job is to
take source code in plain-text format as input and output so-called tokens.
These tokens represent groups of characters that belong together. In this paper's
implementation, each token has a strict type and a text content.
The different types of tokens and other things are defined in the header file
src/interpreter/tokenizer.h.

Here is an example of tokenization:
An input of 'std.test==23.4' would result in the following tokens according
to the tokenizer implementation of this paper:
\begin{enumerate}
    \item Token of type FIELD with content 'std.text'.
    \item Token of type SYMBOL with content '=='.
    \item Token of type NUMBER with content '23.4'.
\end{enumerate}
FIELD type tokens are later replaced by REFERENCE tokens during the stage of localization
and SYMBOL type tokens are mapped to EXPR by src/implementation/mappings/mapop.c
during parsing. NUMBER type tokens are replaced by an actual number type, which
is C's 'long double' type as specified in src/interpreter/bytecode.h.

The tokenizer used in this paper is a simple finite-state machine, reading
character by character and determining the token type as it goes.

These tokens are then directly consumed by the parser as described in the next
section.

\subsubsection{The Parser}
The parser, implemented in src/interpreter/parser.c, generates a syntax-tree
from tokens. The algorithm used is inspired by the Pratt Parsing\footcite{pratt}
algorithm first described by Vaughan Pratt in 1973, which is a kind of precedence
parser based on recursive descent. There is a great simple explanation of the algorithm
by Jonathan Apodaca on dev.io\footcite{devio} linked in the references on page 
\pageref{bibliography} which this paper's parser takes inspiration from.

The syntax tree in the implementation of \name consists of 'stnode' type
pointers, as defined in src/interpreter/parser.h. Each such node depending
on its 'nodetype', declared in src/interpreter/tokenizer.h,
is either a value-node storing a pointer to some data, e.g. a number or text, or a
parent-node referencing two child nodes and an operator, e.g. two numbers as
children and a plus-operator to signify an addition.

The difficult part of implementing a parser is precedence. Precedence and
associativity define the order of execution of statements. A simple example
would be to perform multiplication before addition, e.g. '2+3*4' would
result in 14 and is not the same as '(2+3)*4'. Therefore, each operator is
assigned a precedence, or priority, as well as an associativity, which is
either 'Left to Right' or 'Right to Left'. An example of associativity is
\emph{'20 / 10 / 2'}. Both in C as well as in \name, the division operator is
left-to-right associative, meaning the stated expression is the same as
\emph{'(20 / 10) / 2'} and not as \emph{'20 / (10 / 2)'}. Both precedence 
and associativity are properties that are defined in the file mapop.gpref for
each operator.

The parser implementation in src/interpreter/parser.c may be difficult to
read and understand, which is why the most important functions are explained here:
\begin{itemize}
    \item \emph{advance.} This function reads a new token from the tokenizer.
    \item \emph{secondary.} The 'secondary' function deals with parsing single,
        simple values that make up the start of an expression, such as numbers,
        fields, strings and it handles brackets, such as indexing with square
        brackets, functions or code blocks with curly braces and sub-expressions
        in standard brackets.
    \item \emph{expr.} The 'expr' function calls 'secondary' which acts
        as the basis for a new expression. It then repeatedly checks
        if an operator follows the secondary token and creates a tree of
        EXPR type stnodes, each having children which are recursively parsed
        and information about the operator.
    \item \emph{parse.} The parse function is a small wrapper which only
        calls the 'expr' function, initiating the parsing process.
\end{itemize}

Following is a piece of pseudocode to better illustrate the inner workings of the
parser which is based on a Pratt-parser. 
This is only an example and not completely accurate to the actual parser used
by this project.
TODO: validate code
\code{pseudocode/parser.c}{1}{999}

\subsubsection{The Bytecode Compiler}
The bytecode compiler is part of the interpreter. Its job is to create a list
of byte-sized instructions and data from the syntax-tree the parser created.
Having such an intermediary bytecode format, instead of directly interpreting
the parse-tree can be beneficial. This is the case when the same piece of code
has to be run several times. Instead of generating a time and memory overhead 
having to walk along tree-nodes recursively, only the list of instructions in
the bytecode can be traversed, which is much faster. 

Not to be confused with machine-native bytecode, the bytecode compiler in \name
generates byte-sequences in an internal format, that are then interpreted. A
JIT compiler would instead compile the AST down to machine-native bytecode, which
could run directly on the system's hardware and would not have to be interpreted.

To generate such a sequence of bytes, \name recursively reads through the
binary tree created by the parser and pushes found instructions onto a stack,
which is in this case used similarly to a variable-sized list.
The algorithm used is inspired by Edsger Dijkstra's 'Shunting yard 
algorithm'\footcite{shunting_yard}, traversing the tree post-order to create
a prefix notation. 

A prefix notation is a way to write down mathematical expressions, except
operators precede their operands, in contrast to the more common infix notation,
where operands surround the operator.

In src/interpreter/bytecode.c we can see, that first, the left operand and then
the right operand are added onto the stack of bytecode. Therefore, when
popping from the stack and evaluating instructions, the operands will have to
be read in reverse.
Here is an example:
\begin{enumerate}
    \item Assume the following mathematical expression in infix notation:
        \begin{center}
            \emph{'1 - 2 * 4 / (5 + 6)'}
        \end{center}
    \item In common prefix notation, also called Polish notation, the
        operators would now precede the operands as such:
        \begin{center}
            \emph{'- 1 (/ (* 2 4) (+ 5 6))'}
        \end{center}
    \item But in \name the operands are pushed onto the stack is reverse order.
        This would look like the following:
        \begin{center}
            \emph{'- (/ (+ 6 5) (* 4 2)) 1'}
        \end{center}
    \item The brackets are not needed, because for every operator, the amount
        of operands is known. This is the case as e.g. the subtraction operator
        and the negative operator, which are both a minus sign, are distinguished
        beforehand. This means the above statement would look like this:
        \begin{center}
            \emph{'- / + 6 5 * 4 2 1'}
        \end{center} 
        This is an ideal abstraction of how data 
        is represented internally in \name.
\end{enumerate}

\subsubsection{The Runtime}
This is the last stage of interpreting program code as implemented in \name.
The runtime steps through the intermediary bytecode created by the bytecode
compiler and evaluates expressions by plugging in the provided values.
All available expressions are implemented in 
src/interpreter/processing/implementations.c.

The evaluation of expressions also works in a recursive manner. Here is an 
example of \name's bytecode notation and how it would be evaluated at runtime:
\begin{itemize}
    \item Assume the input in \name's bytecode to resemble the following expression: 
        \begin{center}
            \emph{'- 6 * 3 4'}
        \end{center}
        In infix notation, this would be equivalent to 
        \begin{center}
            \emph{'(4 * 3) - 6'}.
        \end{center}
    \item The runtime will read through the bytecode left-to-right. First it will
        notice the subtraction-operator and accordingly assume that two
        operands must follow.
    \item The processing function will call itsself
        and notice the 6, a number, and therefore a value. The function simply returns the 6
        to the subtraction expression. This 6 will be the right-hand operator
        of the subtraction.
    \item The subtraction will then call the processing function again to find
        the left operand. As the processing function reads a multiplication operator,
        it now recursively reads through that expression's right and then left
        operator, which would be 3 and for respectively and compute and return
        the result of the multiplication, which is 12.
    \item Now the subtraction knows its right- and left-hand operators, 6 and 12,
        and again, calculate and return the result, which is 6.
\end{itemize}

That's it! With all the possible expressions available, entire programs
can now be interpreted without a problem.

\section{Results}
\subsection{Limitation}
\name is a programming language without a doubt. But it is not perfect in any
way. There are limitations that make it practically unusable for actual
professional projects. In this section, a few shortcomings of \name are listed
and reasoned over.

\subsubsection{Memory Leaks}
Memory leaks happen, when a program reserves memory to store some data, but
never frees up the space for use for other programs, even though the data
has long been discarded and is of no further use.
In \name, memory leaks are a major problem. Even though barely noticable for small
programs it interprets, \name does not have proper setups in place to free up
all used memory that is not used any longer.

Other programming languages, notably object-oriented languages such as Java,
use Garbage-Collection, which automatically tracks unused allocated memory and
frees it up automaticadlly. While this is also the case with \name, there are
many internal flaws that have nothing to do with variables used in a \name
program, but rather internally. 

A major reason for the memory leaks, but not an excuse, is the nature of the
C programming language the \name interpreter was written in. The C language and
compiler do not keep track of states in which certain blocks of memory are
forgotten about and warn the developer, instead, memory leaks happen.
More modern C alternatives such as Rust have a ownership-system in place,
in which any allocated memory has a function as an owner, and when the function
is done processing, the memory is automatically freed. Such systems are a
helpful tool for developers to create better programs with low mental overhead,
and are what programming language design should be all about.

\subsubsection{Single-Threaded}
The \name interpreter does not implement any multi-threaded, meaning parallel
running, programs as most other languages do. Instead, any \name program can
only be interpreted and run sequentially. Since most modern computer processors
are equipped with a handful of processing cores and threads, not being able to
make use of those is a major downside and in itsself a reason not to use this
language.

\subsubsection{Token input size}
In the current iteration, each token's content's size is limited to 256 bytes,
which makes it impossible to use e.g. variable names longer thatn 256 ASCII
characters. This would probably be the easyest thing to fix of here-listed 
problems of \name, but since 256 characters are long enough for 
illustration purposes, fixing this is not worth spending time on.

\subsubsection{Lack of a Development Environment}
This is certainly the greatest problem of \name, aside the countless smaller
bothering aspects.

Pretty much any good, modern programming language does not only consist
of it's specification and a compiler or interpreter, but of many more
tools surrounding it.
Such tools often include a package-manager, with which people around the world
can share code. Examples would be npm\footcite{npm} or cargo\footcite{cargo}, 
the 'Node Package Manager' for JavaScript applications and and Rust's package 
manager respectively.

\section{Discussion}

\subsection{Interesting Ideas untouched}
Creating the ideal programming language takes a lot of planning and sketching
out. This section of the paper gives insight on interesting ideas or concepts
that are great to have and are used in other programming languages, but not 
could not be implemented into \name.

\subsubsection{Foreign Function Interfaces}
A foreign function interface\footcite{FFI} often abbreviated as FFI,
is a mechanism by which one programming language can call functions written 
in another. This makes a language more attractive, as it can be used with
existing libraries from another language, instead of functions having to be
re-written. Enabling C function calls often allows a certain language to
directly interact with system libraries or the operating system itsself.

Foreign function interfaces are available in many modern languages.
Examples are Java, Python and Rust.
Java enables users to call C, C++ and even assembly code with the Java Native
Interface\footcite{JNI}, JNI for short, whereas within Python C function can
be run with the standard ctypes\footcite{ctypes} library.

Foreign function interfaces are specially difficult to implement, or implement
right. Let us assume a programming language interpreter \name written in C that
would like to call some functions that are part of yet another C program or
library. The problem lies in the fact that at runtime, C does not know
structure of the functions to be called, say return- or argument-types, which
is by the way where the term foreign functions originates from. This is why the
\name developer would have to provide that information by re-declaring the
function inside \name, so that, with some trickery, data can be passed to and
received from the function in the correct format, preventing a segmentation
fault. The libffi\footcite{libffi} C-library, which is also used by Java and
Python for this purpose, can be used to load and call such foreign functions.

\subsubsection{Reflection}
Reflection\footcite{reflection}, also called reflective programming, describes
the ability of a program to examine and modify its own structure and behavior 
at runtime. An example would be self-modifying code. This can easily be
achieved in assembly language, which inherently does not differentiate between
data and instructions.
Another example is Java, providing methods and classes in the java.lang.reflect
package that enable developers to examine and change properties or functions
of objects.
The advantage of reflection over not having it is that it allows for non-static
programs and program flows, effectively making smaller executables and more
resource efficient programs, as those programs can adjust themselves to run
differently under different conditions at runtime.
The easiest and simplest way to go about implementing Reflection in \name is
certainly to allow the interpretation of strings at runtime. That way, strings
holding program instructions can be created and modified and later executed.

\section{Index}
\section{Closing Words}
\section{Declaration of Autonomy}
Hereby I, Florian Donnelly, declare to have written this paper, also including
the entire source code of \name by myself using resources listed under the
'references' section on page \pageref{bibliography}.

\section{Appendix}

\newpage\printbibliography[heading=bibintoc]\label{bibliography}

TODO: check correctness of all file paths!! e.g. src/...
\end{document}
